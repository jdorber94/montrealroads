{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Cars vs. Bikes Data Gatherer. Find where in your city biking is superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import googlemaps\n",
    "from datetime import datetime\n",
    "gmaps = googlemaps.Client(key='YOUR_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block of code creates a grid of points within a geographic boundary which is inputted as a kml file Default: Island of Montreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of points\n",
    "import os\n",
    "import fiona\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the KML file using geopandas\n",
    "gdf = gpd.read_file('montreal_boundaries.kml', driver='KML')\n",
    "\n",
    "# Find the largest polygon in the GeoDataFrame\n",
    "largest_polygon = gdf['geometry'].iloc[gdf['geometry'].area.idxmax()]\n",
    "\n",
    "# Create a list to store the generated points\n",
    "points_within_polygon = []\n",
    "\n",
    "# Define the number of points you want to generate along each axis\n",
    "num_points_x = 32\n",
    "num_points_y = 32\n",
    "\n",
    "# Generate points in a grid within the bounding box of the largest polygon\n",
    "minx, miny, maxx, maxy = largest_polygon.bounds\n",
    "step_x = (maxx - minx) / (num_points_x - 1)\n",
    "step_y = (maxy - miny) / (num_points_y - 1)\n",
    "\n",
    "for i in range(num_points_x):\n",
    "    for j in range(num_points_y):\n",
    "        x = minx + i * step_x\n",
    "        y = miny + j * step_y\n",
    "        point = Point(x, y)\n",
    "        if point.within(largest_polygon):\n",
    "            points_within_polygon.append(point)\n",
    "\n",
    "points_kml = 'points.kml'\n",
    "# Check if the file exists before attempting to delete it\n",
    "if os.path.exists(points_kml):\n",
    "    # Delete the file\n",
    "    os.remove(points_kml)\n",
    "    print(f\"'{points_kml}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"'{points_kml}' does not exist.\")\n",
    "\n",
    "# Optionally, you can save the generated points as a GeoDataFrame\n",
    "gdf_points = gpd.GeoDataFrame(geometry=points_within_polygon, crs=gdf.crs)\n",
    "gdf_points.crs = 'EPSG:4326'\n",
    "\n",
    "gdf_points.to_file(points_kml, driver='KML')\n",
    "print(points_within_polygon)\n",
    "print(len(points_within_polygon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to convert a GeoJSON to a kml file. This is already done but may be useful for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert montreal GeoJSON to kml\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the GeoJSON file\n",
    "geojson_file = 'limites-terrestres.geojson'\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "# Specify the output KML file\n",
    "kml_file = 'montreal_boundaries.kml'\n",
    "\n",
    "# Save the GeoDataFrame to KML\n",
    "gdf.to_file(kml_file, driver='KML')\n",
    "\n",
    "print(f'Successfully converted {geojson_file} to {kml_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the function call_gmaps. The function takes two places, a time, and a mode of transit, and outputs the trip duration, distance, and parsed route data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gmaps(place1,place2,time,mode_of_transit):\n",
    "    \n",
    "   \n",
    "    data = gmaps.directions(place1,\n",
    "                                     place2,\n",
    "                                     mode=mode_of_transit,\n",
    "                                     departure_time=time)\n",
    "    data=data[0]\n",
    "\n",
    "    parsed_data = json.loads(json.dumps(data))\n",
    "    print(parsed_data)\n",
    "\n",
    "    duration_text = parsed_data['legs'][0]['duration']['text']\n",
    "    duration_value = parsed_data['legs'][0]['duration']['value']\n",
    "\n",
    "    # Print the duration\n",
    "    print(f\"Duration of the trip: {duration_text}\")\n",
    "    print(f\"Duration value (in seconds): {duration_value}\")\n",
    "\n",
    "    # Access the \"distance\" for the entire trip\n",
    "    total_distance_text = parsed_data['legs'][0]['distance']['text']\n",
    "    total_distance_value = parsed_data['legs'][0]['distance']['value']\n",
    "\n",
    "    # Print the total distance\n",
    "    print(f\"Total distance of the trip: {total_distance_text}\")\n",
    "    print(f\"Total distance value (in meters): {total_distance_value}\")\n",
    "    \n",
    "    return duration_value,total_distance_value,parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below script is what calls the API to produce the data comparing biking and driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating distance to downtown not to everyother point\n",
    "from varname import nameof\n",
    "from datetime import datetime,timedelta\n",
    "import pickle\n",
    "\n",
    "num_points = len(points_within_polygon)\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Find the next Thursday\n",
    "days_until_next_thursday = (3 - current_datetime.weekday() + 7) % 7\n",
    "next_thursday = current_datetime + timedelta(days=days_until_next_thursday)\n",
    "\n",
    "# Set the time to 4 PM\n",
    "next_thursday_4pm = next_thursday.replace(hour=16, minute=0, second=0, microsecond=0)\n",
    "time = next_thursday_4pm\n",
    "\n",
    "w = [[0 for _ in range(4)] for _ in range(num_points)]\n",
    "b = [[0 for _ in range(4)] for _ in range(num_points)]\n",
    "d = [[0 for _ in range(4)] for _ in range(num_points)]\n",
    "t = [[0 for _ in range(4)] for _ in range(num_points)]\n",
    "\n",
    "mode_dict = {\n",
    "    \"w\":\"walking\",\n",
    "    \"b\":\"bicycling\",\n",
    "    \"d\":\"driving\",\n",
    "    \"t\": \"transit\"\n",
    "}\n",
    "\n",
    "#This function fills one of the nested lists above with data for each mode of transit\n",
    "def fill_base2(abc,mode,num_points,points_within_polygon):\n",
    "    for i in range(0,(num_points)):\n",
    "        print(i)\n",
    "        place1 = (points_within_polygon[i].y,points_within_polygon[i].x)\n",
    "        #abc[i][0][0] = place1 \n",
    "        place2 = (45.50191640802175, -73.56742138580896) #Centre of montreal\n",
    "        abc[i][0] = place1\n",
    "        abc[i][1] = place2\n",
    "        abc[i][2:5] = call_gmaps(place1,place2,time,mode) #Note the number of times you call this as it costs you\n",
    "        # for j in range(0,(num_points)):\n",
    "        #     print(j)\n",
    "        #     if j != i:\n",
    "        #         place2 = (points_within_polygon[j].y,points_within_polygon[j].x) \n",
    "        #         abc[i][j][0] = place1\n",
    "        #         abc[i][j][1] = place2\n",
    "        #         if j < i:\n",
    "        #             abc[i][j][2:5] = abc[j][i][2:5]\n",
    "        #         else:\n",
    "        #             abc[i][j][2:5] = call_gmaps(place1,place2,time,mode)\n",
    "    return(abc)  \n",
    "\n",
    "#Function is called and all data is saved      \n",
    "mode = mode_dict[nameof(d)]\n",
    "d = fill_base2(d,mode,num_points,points_within_polygon)\n",
    "mode = mode_dict[nameof(b)]\n",
    "b = fill_base2(b,mode,num_points,points_within_polygon)\n",
    "# Save variable to a file\n",
    "with open('b_data2.pkl', 'wb') as file:\n",
    "    pickle.dump(b, file)\n",
    "with open('d_data2.pkl', 'wb') as file:\n",
    "    pickle.dump(d, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below script converts the data calculated above to the data required to produce a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get stats for pickle 2\n",
    "num_points = len(points_within_polygon)\n",
    "# import relevant packages\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import csv\n",
    "import os\n",
    "import fiona\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "\n",
    "#Set variables\n",
    "parking_time = 300 #This is the estimated time it takes to park in seconds downtown\n",
    "\n",
    "# import relevant data\n",
    "mtl_map = gdf\n",
    "# Load the 'w' variable from the file\n",
    "# with open('w_data.pkl', 'rb') as file:\n",
    "#     loaded_w = pickle.load(file)\n",
    "with open('d_data2.pkl', 'rb') as file:\n",
    "    loaded_d = pickle.load(file)\n",
    "with open('b_data2.pkl', 'rb') as file:\n",
    "    loaded_b = pickle.load(file)\n",
    "\n",
    "print(loaded_b[0:num_points][0:num_points][0:2])\n",
    "\n",
    "def get_stats2(loaded_dat_a,loaded_dat_b,num_points):\n",
    "    stats = [[0 for _ in range(0,2)] for _ in range(num_points)]\n",
    "    #for i in range(0,num_points):\n",
    "        #print(loaded_dat_a[0][i][0])\n",
    "    print(stats)    \n",
    "\n",
    "    for i in range(0,num_points):\n",
    "        \n",
    "\n",
    "        stats[i][0] = loaded_dat_a[i][0]\n",
    "        mode_a = loaded_dat_a[i][2]\n",
    "        print(mode_a)\n",
    "        \n",
    "        mode_b = loaded_dat_b[i][2]\n",
    "        print(mode_b)\n",
    "        \n",
    "        stat =  (mode_a+parking_time)/mode_b\n",
    "        \n",
    "        stats[i][1] = stat\n",
    "        print(stats)\n",
    "\n",
    "    #stats = [list(row) for row in zip(*stats)] #transpose stats\n",
    "    stats_dict = conv_dict(stats)\n",
    "    return(stats_dict)\n",
    "\n",
    "stats_dict = get_stats2(loaded_d,loaded_b,num_points)\n",
    "print(stats_dict)\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(stats_dict)\n",
    "\n",
    "# Create a folium map centered around Montreal\n",
    "montreal_map = folium.Map(location=[45.5088, -73.554], zoom_start=12)\n",
    "\n",
    "# Extract intensity values from the dictionary\n",
    "intensity_values = stats_dict['intensity']\n",
    "\n",
    "# Calculate the minimum and maximum intensity\n",
    "min_intensity = min(intensity_values)\n",
    "print(min_intensity)\n",
    "max_intensity = max(intensity_values)\n",
    "print(max_intensity)\n",
    "# Define a custom gradient for the heatmap colors\n",
    "gradient = {min_intensity: 'red', 0.5: 'yellow', 1.0: 'blue', max_intensity: 'green'}\n",
    "\n",
    "# Create a HeatMap layer using the data\n",
    "# Using zip to create rows\n",
    "#heat_data = list(zip(data_dict['latitude'], data_dict['longitude'], data_dict['intensity']))\n",
    "heat_data = [[point[\"latitude\"], point[\"longitude\"], point[\"intensity\"]] for index, point in df.iterrows()]\n",
    "print(heat_data)\n",
    "HeatMap(heat_data, \n",
    "        radius = 5,\n",
    "        min_opacity=0.4,\n",
    "        blur = 3).add_to(montreal_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "montreal_map.save(\"montreal_heatmap.html\")\n",
    "\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file_path = \"heat_data.csv\"\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header if needed\n",
    "    # csv_writer.writerow([\"latitude\", \"longitude\", \"intensity\"])\n",
    "\n",
    "    # Write the data\n",
    "    csv_writer.writerows(heat_data)\n",
    "\n",
    "print(f'Heat data has been saved to {csv_file_path}')\n",
    "\n",
    "# This creates a visualization of all the points greater than 1 which can be plugged into google earth\n",
    "# Create a GeoDataFrame from the DataFrame\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# Filter points where the ratio is above 1\n",
    "gdf_filtered = gdf[gdf['intensity'] > 1]\n",
    "\n",
    "# Specify the output KML file\n",
    "kml_file = 'filtered_points.kml'\n",
    "\n",
    "# Save the filtered GeoDataFrame to KML\n",
    "gdf_filtered.to_file(kml_file, driver='KML')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Load the GeoJSON file containing road polygons\n",
    "gdf_roads = gpd.read_file('roads_halved.geojson')\n",
    "\n",
    "# Load the CSV file containing the grid of data points\n",
    "csv_file = 'heat_data.csv'\n",
    "df_grid = pd.read_csv(csv_file, header=None, names=['latitude', 'longitude', 'data_point'])\n",
    "\n",
    "# Assuming you have columns 'longitude', 'latitude', and 'data_point' in the CSV\n",
    "# If not, replace these column names with your actual column names\n",
    "grid_coordinates = df_grid[['longitude', 'latitude']].values\n",
    "grid_values = df_grid['data_point'].values\n",
    "\n",
    "# Function to interpolate data for a polygon\n",
    "def interpolate_data(polygon, grid_coordinates, grid_values):\n",
    "    polygon_points = np.array(polygon.exterior.coords)\n",
    "    interpolated_values = griddata(grid_coordinates, grid_values, polygon_points, method='linear')\n",
    "    return np.nanmean(interpolated_values)  # Using nanmean to handle possible NaN values\n",
    "\n",
    "# Create a new column 'interpolated_data' in the roads GeoDataFrame\n",
    "gdf_roads['interpolated_data'] = gdf_roads['geometry'].apply(lambda polygon: interpolate_data(polygon, grid_coordinates, grid_values))\n",
    "\n",
    "# Save the GeoDataFrame with interpolated data to a new GeoJSON file\n",
    "gdf_roads.to_file('roads_with_interpolated_data2', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Load the GeoJSON file containing road polygons\n",
    "gdf_roads = gpd.read_file('roads_halved.geojson')\n",
    "\n",
    "# Load the CSV file containing the grid of data points\n",
    "csv_file = 'heat_data.csv'\n",
    "df_grid = pd.read_csv(csv_file, header=None, names=['latitude', 'longitude', 'data_point'])\n",
    "\n",
    "# Assuming you have columns 'longitude', 'latitude', and 'data_point' in the CSV\n",
    "# If not, replace these column names with your actual column names\n",
    "grid_coordinates = df_grid[['longitude', 'latitude']].values\n",
    "grid_values = df_grid['data_point'].values\n",
    "\n",
    "# Function to interpolate data for a geometry\n",
    "def interpolate_data(geometry, grid_coordinates, grid_values):\n",
    "    if geometry.type == 'Polygon':\n",
    "        polygon_points = np.array(geometry.exterior.coords)\n",
    "    elif geometry.type == 'LineString':\n",
    "        polygon_points = np.array(geometry.coords)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported geometry type\")\n",
    "\n",
    "    interpolated_values = griddata(grid_coordinates, grid_values, polygon_points, method='linear')\n",
    "    return np.nanmean(interpolated_values)  # Using nanmean to handle possible NaN values\n",
    "\n",
    "# Create a new column 'interpolated_data' in the roads GeoDataFrame\n",
    "gdf_roads['interpolated_data'] = gdf_roads['geometry'].apply(lambda geom: interpolate_data(geom, grid_coordinates, grid_values))\n",
    "\n",
    "# Save the GeoDataFrame with interpolated data to a new GeoJSON file\n",
    "gdf_roads.to_file('roads_with_interpolated_data2.geojson', driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important function. Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_dict(stats):\n",
    "    data = {\n",
    "        \"latitude\": [],\n",
    "        \"longitude\": [],\n",
    "        \"intensity\": []\n",
    "    }\n",
    "\n",
    "    # Iterate through the stats list and populate the data dictionary\n",
    "    for point, intensity in stats:\n",
    "        latitude, longitude = point\n",
    "        data[\"latitude\"].append(latitude)\n",
    "        data[\"longitude\"].append(longitude)\n",
    "        data[\"intensity\"].append(intensity)\n",
    "\n",
    "    # Print the resulting data dictionary\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from shapely.geometry import LineString, Polygon, mapping\n",
    "\n",
    "# Load your GeoJSON data\n",
    "with open('roads_with_interpolated_data2.geojson', 'r') as geojson_file:\n",
    "    geojson_data = json.load(geojson_file)\n",
    "\n",
    "# Function to convert LineString to Polygon\n",
    "def convert_line_to_polygon(line_string):\n",
    "    coordinates = line_string['coordinates']\n",
    "    # Close the LineString to create a ring\n",
    "    coordinates.append(coordinates[0])\n",
    "    polygon = Polygon(coordinates)\n",
    "    return mapping(polygon)\n",
    "\n",
    "# Iterate through features and convert LineString to Polygon\n",
    "for feature in geojson_data['features']:\n",
    "    if feature['geometry']['type'] == 'LineString':\n",
    "        feature['geometry'] = convert_line_to_polygon(feature['geometry'])\n",
    "\n",
    "# Save the modified GeoJSON with Polygons\n",
    "with open('output_geojson_with_polygons.geojson', 'w') as output_geojson:\n",
    "    json.dump(geojson_data, output_geojson, indent=2)\n",
    "\n",
    "print(\"Conversion complete. GeoJSON with Polygons saved to output_geojson_with_polygons.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from shapely.geometry import LineString, Polygon, mapping\n",
    "\n",
    "# Load your GeoJSON data\n",
    "with open('roads_with_interpolated_data2.geojson', 'r') as geojson_file:\n",
    "    geojson_data = json.load(geojson_file)\n",
    "\n",
    "# Function to convert LineString to Polygon\n",
    "def convert_line_to_polygon(line_string):\n",
    "    coordinates = line_string['coordinates']\n",
    "    # Close the LineString to create a ring\n",
    "    coordinates.append(coordinates[0])\n",
    "    polygon = Polygon(coordinates)\n",
    "    return mapping(polygon)\n",
    "\n",
    "# Iterate through features and convert LineString to Polygon\n",
    "for feature in geojson_data['features']:\n",
    "    if feature['geometry']['type'] == 'LineString':\n",
    "        feature['geometry'] = convert_line_to_polygon(feature['geometry'])\n",
    "\n",
    "# Save the modified GeoJSON with Polygons\n",
    "with open('output_geojson_with_polygons.geojson', 'w') as output_geojson:\n",
    "    json.dump(geojson_data, output_geojson, indent=2)\n",
    "\n",
    "print(\"Conversion complete. GeoJSON with Polygons saved to output_geojson_with_polygons.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "def simplify_geometry(geometry, tolerance=0.0001):\n",
    "    if geometry.geom_type == 'LineString':\n",
    "        simplified_geometry = LineString(geometry.simplify(tolerance))\n",
    "        return simplified_geometry\n",
    "    return geometry\n",
    "\n",
    "def reduce_geojson_size(input_geojson, output_geojson, tolerance=0.0001):\n",
    "    # Load the GeoJSON file\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Apply simplification to each geometry in the GeoDataFrame\n",
    "    gdf['geometry'] = gdf['geometry'].apply(lambda geom: simplify_geometry(geom, tolerance))\n",
    "\n",
    "    # Save the simplified GeoDataFrame to a new GeoJSON file\n",
    "    gdf.to_file(output_geojson, driver='GeoJSON')\n",
    "\n",
    "    print(f\"GeoJSON size reduced. Result saved to {output_geojson}\")\n",
    "\n",
    "# Example usage\n",
    "input_geojson_file = \"output_geojson_with_polygons.geojson\"  # Replace with the path to your input GeoJSON\n",
    "output_geojson_file = \"roads_reduced_polygon.geojson\"  # Replace with the desired output path\n",
    "\n",
    "reduce_geojson_size(input_geojson_file, output_geojson_file, tolerance=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def remove_properties(input_geojson, output_geojson):\n",
    "    # Load GeoJSON file\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Remove properties for each feature\n",
    "    for index, row in gdf.iterrows():\n",
    "        gdf.at[index, 'properties'] = {}\n",
    "\n",
    "    # Save modified GeoJSON\n",
    "    gdf.to_file(output_geojson, driver='GeoJSON')\n",
    "\n",
    "    print(f'Properties removed and GeoJSON saved to {output_geojson}')\n",
    "\n",
    "# Example usage:\n",
    "input_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\geobase.json\"\n",
    "#input_geojson_file = \"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\geobase.json\"\n",
    "output_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\roads_size_reduce.geojson\"\n",
    "\n",
    "remove_properties(input_geojson_file, output_geojson_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def remove_properties(input_geojson, output_geojson):\n",
    "    # Load GeoJSON file\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Set an empty dictionary for the 'properties' column for all rows\n",
    "    gdf['properties'] = [{} for _ in range(len(gdf))]\n",
    "\n",
    "    # Save modified GeoJSON\n",
    "    gdf.to_file(output_geojson, driver='GeoJSON')\n",
    "\n",
    "    print(f'Properties removed and GeoJSON saved to {output_geojson}')\n",
    "\n",
    "# Example usage:\n",
    "input_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\geobase.json\"\n",
    "output_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\roads_size_reduce.geojson\"\n",
    "\n",
    "remove_properties(input_geojson_file, output_geojson_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def remove_shapes_at_edges(input_geojson, output_geojson):\n",
    "    # Load GeoJSON file\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Create a buffer around the entire geometry\n",
    "    buffer_distance = 0.01  # Adjust the buffer distance as needed\n",
    "    buffered_geometry = gdf.unary_union.buffer(buffer_distance)\n",
    "\n",
    "    # Remove shapes at the edges by taking the difference\n",
    "    gdf = gdf.difference(buffered_geometry)\n",
    "\n",
    "    # Save modified GeoJSON\n",
    "    gdf.to_file(output_geojson, driver='GeoJSON')\n",
    "\n",
    "    print(f'Shapes at edges removed, and GeoJSON saved to {output_geojson}')\n",
    "\n",
    "# Example usage:\n",
    "input_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\roads_size_reduce.geojson\"\n",
    "output_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\roads_no_edges.geojson\"\n",
    "\n",
    "remove_shapes_at_edges(input_geojson_file, output_geojson_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, MultiLineString\n",
    "\n",
    "def cut_geojson_in_half(input_geojson, output_geojson):\n",
    "    # Load GeoJSON file\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Find the bounding box center\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    center_x, center_y = (minx + maxx) / 2, (miny + maxy) / 2\n",
    "\n",
    "    # Create a vertical line through the center\n",
    "    center_line = LineString([(center_x, miny), (center_x, maxy)])\n",
    "\n",
    "    # Split geometries into two based on the center line\n",
    "    split_geoms = []\n",
    "    for geom in gdf['geometry']:\n",
    "        if geom.intersects(center_line):\n",
    "            split_geoms.extend(geom.difference(center_line).geoms)\n",
    "        else:\n",
    "            split_geoms.append(geom)\n",
    "\n",
    "    # Create a new GeoDataFrame with the split geometries\n",
    "    split_gdf = gpd.GeoDataFrame(geometry=split_geoms, crs=gdf.crs)\n",
    "\n",
    "    # Save the split GeoJSON\n",
    "    split_gdf.to_file(output_geojson, driver='GeoJSON')\n",
    "\n",
    "    print(f'GeoJSON split in half and saved to {output_geojson}')\n",
    "\n",
    "# Example usage:\n",
    "input_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\roads_size_reduce.geojson\"\n",
    "output_geojson_file = r\"C:\\Users\\steve\\Cloud-Drive\\Car free centre ville\\roads_no_edges.geojson\"\n",
    "\n",
    "cut_geojson_in_half(input_geojson_file, output_geojson_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "\n",
    "# Load the grid data (assuming no column names and columns in order: latitude, longitude, ratio)\n",
    "grid_data = pd.read_csv('heat_data.csv', header=None, names=['latitude', 'longitude', 'ratio'])\n",
    "\n",
    "# Assuming you have a GeoDataFrame for Montreal city boundaries\n",
    "montreal_boundaries = gpd.read_file('limites-terrestres.geojson')\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "geometry = [Polygon([(row[1], row[0]), (row[1] + 1, row[0]), (row[1] + 1, row[0] + 1), (row[1], row[0] + 1)]) for _, row in grid_data.iterrows()]\n",
    "grid_data_gdf = gpd.GeoDataFrame(grid_data, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# Set the GeoDataFrame's CRS\n",
    "grid_data_gdf.crs = 'EPSG:4326'\n",
    "\n",
    "# Filter grid data where the ratio is greater than one\n",
    "filtered_grid_data = grid_data_gdf[grid_data_gdf['ratio'] > 1]\n",
    "\n",
    "# Intersect the polygons with Montreal boundaries to clip the result within Montreal\n",
    "result = gpd.overlay(filtered_grid_data, montreal_boundaries, how='intersection')\n",
    "\n",
    "\n",
    "# Save the result to a GeoJSON file\n",
    "result.to_file('biking_boundary.geojson', driver='GeoJSON')\n",
    "\n",
    "print(\"Shape created for areas where the ratio is greater than one. Result saved to result.geojson.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
